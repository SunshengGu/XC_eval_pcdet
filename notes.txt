build_network:
- In tools/test.py and tools/train.py, the model is built by build_network()
- This function is found in pcdet/models/__init__.py
- Which calls build_detector to create a model

build_detector:
- Found in pcdet/models/detectors/__init__.py
- Picks the right model based on model_cfg.NAME, and builds the model by passing model_cfg, num_class, dataset
- All detector models are defined in pcdet/models/detectors, and have Detector3DTemplate as the parent class
- Parent class of Detector3DTemplate is torch.nn.Module (proved that the model is indeed a torch nn)

pointpillar.py:
- self.module_list = self.build_networks(), this returns a list of modules
- so, each detector is just a module that contains a bunch of modules
- getattr(object, name[, default]): Return the value of the named attribute of object. name must be a string
- self.module_topology: the elements of which can be used one by one to build 'vfe', 'backbone_3d', 'map_to_bev_module', 'pfe', 'backbone_2d', 'dense_head',  'point_head', 'roi_head'
- Take vfe for example, it is in the folder pcdet/models/vfe
- For all vfe's, the base class is VFETemplate, which inherits from nn.Module
- forward(batch_dict): feed the batch_dict in a sequential fashion through the network to get losses and stuff
    - return: pred_dicts, recall_dicts = self.post_processing(batch_dict) (in the case of testing)

detector post_processing():
- defined in pcdet/models/detectors/detector3d_template.py

training loss calculation:
- Calculated by the get_loss function in pcdet/models/dense_heads/anchor_head_template.py
- classification loss computed by self.get_cls_layer_loss()
***
- box class labels are in self.forward_ret_dict['box_cls_labels']
***
- forward_ret_dict is filled in pcdet/models/dense_heads/anchor_head_single.py
- ground truth comes from data_dict['gt_boxes']

Input for XAI:
- should be the same as the input for the first part of the network
***
- easier version: do explanation for the detection head and the 2D backbone, ignore VFE and BEV stuff for now
- so the input should really be the input to the 2D backbone: data_dict['spatial_features']
- see pcdet/models/backbones_2d/base_bev_backbone.py
***

dataset:
- In both tools/test.py and tools/train.py, dataset is prepared by build_dataloader()
- This function is defined in pcdet/datasets/__init__.py
- dataset is created by taking the following as inputs and feed them to the dataset py file: 
		dataset_cfg=dataset_cfg,
        class_names=class_names,
        root_path=root_path,
        training=training,
        logger=logger,
- Each dataset has a corresponding py file in pcdet/datasets folder, and has a class corresponding to the dataset
- Each dataset class has DatasetTemplate as the base class, defined in pcdet/datasets
- DatasetTemplate inherites from torch_data.Dataset(torch_data is alias of torch.utils.data)
- torch.utils.data.Dataset: All datasets that represent a map from keys to data samples should subclass it
- DatasetTemplate: set up the dataset based on settings in the config files

test_loader:
- In both tools/test.py and tools/train.py, dataset is prepared by build_dataloader()
- This function is defined in pcdet/datasets/__init__.py
- DataLoader is from torch.utils.data: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader

Sampler:
- An instance of torch.utils.data.distributed.DistributedSampler
- Sampler that restricts data loading to a subset of the dataset
- Not used in test.py

eval_single_ckpt: (are the methods of KittiDataset such as `get_lidar` being used here?)
- In test.py
- load_params_from_file(): defined in pcdet/models/detectors/detector3d_template.py
    - Uses a state_dict to store model states in a ckpt file: https://pytorch.org/tutorials/beginner/saving_loading_models.html
    - Updates the model's state_dict() by calling self.load_state_dict(state_dict)
    - State_dict is a dict, both the model and the optimizer have a state_dict
- cuda(): Moves all model parameters and buffers to the GPU
- eval_utils.eval_one_epoch():
    - In tools/eval_utils
    - These lines show how the training is done:
        for i, batch_dict in enumerate(dataloader):
            load_data_to_gpu(batch_dict)
            with torch.no_grad():
                pred_dicts, ret_dict = model(batch_dict)

Target assigner:
- See pcdet/models/dense_heads/anchor_head_single.py, self.assign_targets()
- Pointpillar uses AxisAlignedTargetAssigner, which takes the following as inputs:
        anchor_target_cfg=anchor_target_cfg,
        anchor_generator_cfg=anchor_generator_cfg,
        class_names=self.class_names,
        box_coder=self.box_coder,
        match_height=anchor_target_cfg.MATCH_HEIGHT
- The assign_target method is defined in pcdet/models/dense_heads/target_assigner/axis_aligned_target_assigner.py
- How to get class labels and box info from the ground truth boxes:
        gt_classes = gt_boxes_with_classes[:, :, 7]
        gt_boxes = gt_boxes_with_classes[:, :, :7]

cfg:
- In test.py and train.py, cfg came from pcdet.config, but filled by cfg_from_yaml_file(args.cfg_file, cfg) in parse_config()
- Get config from yaml: cfg_from_yaml_file(), defined in pcdet/config.py
- cfg is an EasyDict instance, EasyDict allows to access dict values as attributes (works recursively).

torch.nn.Module
- Base class for all neural network modules

How does PointPillar process KITTI lidar data:
- Use test.py as example, `test_set` is the first thing returned by the build_dataloader() function
- build_dataloader() is defined in pcdet/datasets/__init__.py
- The first return value, `dataset`, is created by the KittiDataset class in the case of KITTI
- KittiDataset is defined in pcdet/dataset/kitti/kitti_dataset.py, and inherits from DatasetTemplate, which in term inherits from torch_data.Dataset
- The KittiDataset class basically learns the settings in kitti_dataset.yaml and pointpillar.yaml

From: https://docs.python.org/3/tutorial/modules.html
- The __init__.py files are required to make Python treat directories containing the file as packages.
- Note that when using from package import item, the item can be either a submodule (or subpackage) of the package, or some other name defined in the package, like a function, class or variable
- Contrarily, when using syntax like import item.subitem.subsubitem, each item except for the last must be a package; the last item can be a module or a package but can’t be a class or function or variable defined in the previous item.
- The import statement uses the following convention: if a package’s __init__.py code defines a list named __all__, it is taken to be the list of module names that should be imported when from package import * is encountered.

Train command (note: the --cfg_file arg has to start with the `cfg` folder or else the outout will be in some weird place):
- cadc: python train.py --cfg_file cfgs/cadc_models/pointpillar.yaml --batch_size 4 --epochs 4

Test command:
- kitti: python test.py --cfg_file cfgs/kitti_models/pointpillar.yaml --batch_size 4 --ckpt ~/pcdet/output/kitti_models/pointpillar/default/ckpt/pointpillar_7728.pth
- deformable pvrcnn: python test.py --cfg_file cfgs/kitti_models/def_pv_rcnn.yaml --batch_size 4 --ckpt /pt/Deformable-PV-RCNN/OpenPCDet/output/def_pv_rcnn/def_pv_rcnn_all.pth
- cadc: python test.py --cfg_file cfgs/cadc_models/pointpillar.yaml --batch_size 4 --ckpt ~/pcdet/output/cadc_models/pointpillar/default/ckpt/checkpoint_epoch_4.pth

Test with 2 GPUs:
- bash scripts/dist_test.sh 4 \
    --cfg_file ~/pcdet/tools/cfgs/kitti_models/pointpillar.yaml --batch_size 4 --ckpt ~/pcdet/output/kitti_models/pointpillar/default/ckpt/checkpoint_epoch_2.pth

XAI command:
- kitti, old (before decoupling): python XAI.py --cfg_file cfgs/kitti_models/pointpillar.yaml --explained_cfg_file ~/pcdet/tools/cfgs/kitti_models/pointpillar_2DBackbone_DetHead.yaml --batch_size 1 --ckpt ~/pcdet/output/kitti_models/pointpillar/default/ckpt/pointpillar_7728.pth
- kitti, new (after decoupling): python XAI.py --cfg_file cfgs/kitti_models/pointpillar_xai.yaml --explained_cfg_file cfgs/kitti_models/pointpillar_2DBackbone_DetHead_xai.yaml --batch_size 1 --ckpt ~/pcdet/output/kitti_models/pointpillar/default/ckpt/pointpillar_7728.pth
- cadc, new (after decoupling): python XAI.py --cfg_file cfgs/cadc_models/pointpillar_xai.yaml --explained_cfg_file cfgs/cadc_models/pointpillar_2DBackbone_DetHead_xai.yaml --batch_size 1 --ckpt ~/pcdet/output/cadc_models/pointpillar/default/ckpt/checkpoint_epoch_4.pth

XAI with 2 GPUs:
- bash scripts/dist_explain.sh 2 \
    --cfg_file ~/pcdet/tools/cfgs/kitti_models/pointpillar.yaml --batch_size 4 --explained_cfg_file ~/pcdet/tools/cfgs/kitti_models/pointpillar_2DBackbone_DetHead.yaml --ckpt ~/pcdet/output/kitti_models/pointpillar/default/ckpt/checkpoint_epoch_2.pth

In pcdet/models/dense_heads/anchor_head_single.py, what is the difference between self.forward_ret_dict['cls_preds'] vs data_dict['batch_cls_preds']
-   cls_preds data type: <class 'torch.Tensor'>
    cls_preds shape: torch.Size([4, 248, 216, 18])
    cls_preds[0] shape: torch.Size([248, 216, 18])
    box_preds data type: <class 'torch.Tensor'>
    box_preds shape: torch.Size([4, 248, 216, 42])
    box_preds[0] shape: torch.Size([248, 216, 42])
    batch_cls_preds data type: <class 'torch.Tensor'>
    batch_cls_preds shape: torch.Size([4, 321408, 3])
    batch_cls_preds[0] shape: torch.Size([321408, 3])
    batch_box_preds data type: <class 'torch.Tensor'>
    batch_box_preds shape: torch.Size([4, 321408, 7])
    batch_box_preds[0] shape: torch.Size([321408, 7])

d3_box_overlap in pcdet/datasets/kitti/kitti_object_eval_python/eval.py:
- Creates a list of indices for box parameters
- Delete indices for z location and height
- Calls rotate_iou_gpu_eval from pcdet/datasets/kitti/kitti_object_eval_python/rotate_iou.py
- rotate_iou_gpu_eval seems to calculate 2D IoU
- d3_box_overlap_kernel from pcdet/datasets/kitti/kitti_object_eval_python/eval.py the calculates 3D IoU

Instructions for Martin Ma:

Getting Started:
1. Create a new branch from this `XAI` branch, call it `XAI_Martin`: https://git.uwaterloo.ca/wise-ads/WISEOpenLidarPerceptron/-/tree/XAI
2. Follow instructions here to complete development install for Captum as you will modify its source code
3. Switch your PCDet local repo to the `XAI_Martin` branch and change file paths for the host machine to match your file paths
4. tools/XAI.py is modified from tools/test.py to build a model and use captum's API to explain a particular sample input for the model
5. This contains what I've learned about how PCDet builds the PointPillar network: https://docs.google.com/presentation/d/18HLS68ET3vXomBBOjc_BQIZP1idZ9JCPIZswmoH7eLQ/edit?usp=sharing
6. This document is work-in-progress, will be more polished as I dig deeper
7. notes.txt is a note for myself for understanding PCDet, you may use it as well
8. captum_notes.txt contains what I've learned about captum's Saliency method

The Challenge:
- Currently, I'm trying to generate explanation for the 2D pseudoimage in pointpillar using captum's Saliency method (equivalent to backprop with respect to input pixels)
- (note: you may read sections 2 and 3 of the pointpillar paper https://arxiv.org/pdf/1812.05784.pdf to get a better understanding, but my Google slides has a summary)
- I was able to create two pointpillar models using two different config files, one full pointpillar model (call it full model) and one model (call it 2D model) with just the 2D backbone and detection head (see Fig 2 in the pointpillar paper).
- My plan: run the full model once to create the 2D pseudoimage, then I run Saliency on the 2D model using the 2D pseudoimage as input
- (note: the full model is already trained and has a checkpoint, so don't need to worry about that)
- (note: also, the command for running the XAI.py file is - python XAI.py --cfg_file ~/pcdet/tools/cfgs/kitti_models/pointpillar.yaml --explained_cfg_file ~/pcdet/tools/cfgs/kitti_models/pointpillar_2DBackbone_DetHead.yaml --batch_size 4 --ckpt ~/pcdet/output/kitti_models/pointpillar/default/ckpt/checkpoint_epoch_2.pth)
- (note: no need to change that command unless you want to explain a different model or use a different checkpoint)
- The problem is, PCDet models and captum's APIs use different input formats
- PCDet models take a dictionary as input and will fill the dictionary with additional key-value pairs in each stage (VFE, 2D backbone, deteciton head etc.)
- But captum expects a torch tensor as input and will feed this tensor to the model it tries to explain, it uses a separate argument to take in additional input argument other than the tensor

Sunsheng's Proposed Solution:
- We can try to make the 2D model from PCDet accept a tensor as its primary input, and use a separate dictionary to store additional required data
- In the mean time, changes on the Captum side might be required to be able to process the additional input argument (i.e., the dictionary)

Sunsheng's Tasks:
1. Separate the tensor being fed through the network from the dictionary for pointpillar in PCDet
2. Keep filling the PCDet/PointPillar part in this document: https://docs.google.com/presentation/d/18HLS68ET3vXomBBOjc_BQIZP1idZ9JCPIZswmoH7eLQ/edit?usp=sharing

Martin's Tasks:
1. Thoroughly understand how Captum's Saliency method processes input and creates attributions
2. Summarize your learning in the Captum part in this document: https://docs.google.com/presentation/d/18HLS68ET3vXomBBOjc_BQIZP1idZ9JCPIZswmoH7eLQ/edit?usp=sharing

End Goal:
An attribution heatmap for the 2D model